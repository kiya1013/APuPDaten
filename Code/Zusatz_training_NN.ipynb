{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Training a CNN \n",
    "\n",
    "=====================\n",
    "\n",
    "In this notebook, we use a CNN to train a model on the dataset. <br>\n",
    "The Code is very computationally intensive,<br>\n",
    "and it takes more than 30 min to train the model on GTX970, <br>\n",
    "if we want decent results. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def confusion(prediction, truth):\n",
    "    \"\"\" Returns the confusion matrix for the values in the `prediction` and `truth`\n",
    "    tensors, i.e. the amount of positions where the values of `prediction`\n",
    "    and `truth` are\n",
    "    - 1 and 1 (True Positive)\n",
    "    - 1 and 0 (False Positive)\n",
    "    - 0 and 0 (True Negative)\n",
    "    - 0 and 1 (False Negative)\n",
    "    \"\"\"\n",
    "\n",
    "    confusion_vector = prediction / truth\n",
    "    # Element-wise division of the 2 tensors returns a new tensor which holds a\n",
    "    # unique value for each case:\n",
    "    #   1     where prediction and truth are 1 (True Positive)\n",
    "    #   inf   where prediction is 1 and truth is 0 (False Positive)\n",
    "    #   nan   where prediction and truth are 0 (True Negative)\n",
    "    #   0     where prediction is 0 and truth is 1 (False Negative)\n",
    "\n",
    "    true_positives = torch.sum(confusion_vector == 1).item()\n",
    "    false_positives = torch.sum(confusion_vector == float('inf')).item()\n",
    "    true_negatives = torch.sum(torch.isnan(confusion_vector)).item()\n",
    "    false_negatives = torch.sum(confusion_vector == 0).item()\n",
    "\n",
    "    return true_positives, false_positives, true_negatives, false_negatives"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Load and inspect the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "df = pd.read_csv('..\\\\Data\\\\data_01_clean.csv',index_col='Unnamed: 0')\n",
    "\n",
    "X=df.drop(columns=['Dev'])\n",
    "y= df['Dev']\n",
    "\n",
    "X = X.values # to numpy-array\n",
    "y = y.values # to numpy-array\n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train=scaler.fit_transform(X_train)\n",
    "X_test=scaler.transform(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose(\n",
    "    [transforms.ToTensor(),\n",
    "     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
    "\n",
    "batch_size = 4\n",
    "\n",
    "trainset=list(zip(X_train,y_train))\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size,\n",
    "                                          shuffle=True)\n",
    "\n",
    "testset=list(zip(X_test,y_test))\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=batch_size,\n",
    "                                         shuffle=False)\n",
    "\n",
    "classes = ('normal','Anomaly')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us show some of the training images, for fun.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Define a Convolutional Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class DeepNeuralNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.hidden1 = nn.Linear(39, 4096)\n",
    "        self.hidden2 = nn.Linear(4096, 8192)\n",
    "        self.hidden3 = nn.Linear(8192, 1000)\n",
    "        self.hidden4 = nn.Linear(1000, 100)\n",
    "        self.hidden5 = nn.Linear(100, 2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \n",
    "        z1 = self.hidden1(x)\n",
    "        a1 = torch.sigmoid(z1)\n",
    "        z2 = self.hidden2(a1)\n",
    "        a2 = torch.sigmoid(z2)\n",
    "        z3 = self.hidden3(a2)\n",
    "        a3 = torch.sigmoid(z3)\n",
    "        z4 = self.hidden4(a3)\n",
    "        a4 = torch.sigmoid(z4)\n",
    "        z5 = self.hidden5(a4)\n",
    "        return z5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Train the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "import time\n",
    "\n",
    "net = DeepNeuralNetwork()\n",
    "net = net.float()\n",
    "net.to(device)\n",
    "\n",
    "train_losses = []\n",
    "train_accuracy = []\n",
    "test_accuracy  = []\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(net.parameters(), lr=0.001, momentum=0.9)\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "for epoch in range(10):\n",
    "\n",
    "    running_loss = 0.0\n",
    "    \n",
    "    net.train()\n",
    "    for i, data in enumerate(trainloader, 0):\n",
    "        # get the inputs; data is a list of [inputs, labels]\n",
    "        inputs, labels = data\n",
    "        \n",
    "        inputs = inputs.to(device) #needed when using gpu\n",
    "        labels = labels.to(device) #needed when using gpu\n",
    "\n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # forward + backward + optimize\n",
    "        outputs = net(inputs.float())\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # print statistics\n",
    "        running_loss += loss.item()\n",
    "        if i % 2000 == 1999:    # print every 2000 mini-batches\n",
    "            print(f'[{epoch + 1}, {i + 1:5d}] loss: {running_loss / 2000:.3f}')\n",
    "            train_losses.append(running_loss / 2000)\n",
    "            running_loss = 0.0\n",
    "            \n",
    "        correct = 0\n",
    "        \n",
    "        torch.save(net.state_dict(), f'..\\\\Models\\\\net_e{epoch}.pt')\n",
    "    \n",
    "#---------------------------Test---Testing----------------------------------------------------------\n",
    "    net.eval()   \n",
    "     \n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    # since we're not training, we don't need to calculate the gradients for our outputs\n",
    "    with torch.no_grad():\n",
    "        for data in testloader:\n",
    "            images, labels = data\n",
    "            images = images.to(device) #needed when using gpu\n",
    "            labels = labels.to(device) #needed when using gpu\n",
    "            \n",
    "            # calculate outputs by running images through the network\n",
    "            outputs = net(images.float())\n",
    "            # the class with the highest energy is what we choose as prediction\n",
    "            predicted = torch.argmax(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "            test_accuracy.append(confusion(predicted, labels)+(epoch+1,))\n",
    "\n",
    "    \n",
    "#---------------------------Train---Testing----------------------------------------------------------\n",
    "\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    # since we're not training, we don't need to calculate the gradients for our outputs\n",
    "    with torch.no_grad():\n",
    "        for data in trainloader:\n",
    "            images, labels = data\n",
    "            images = images.to(device) #needed when using gpu\n",
    "            labels = labels.to(device) #needed when using gpu\n",
    "            \n",
    "            # calculate outputs by running images through the network\n",
    "            outputs = net(images.float())\n",
    "            # the class with the highest energy is what we choose as prediction\n",
    "            predicted = torch.argmax(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "            train_accuracy.append(confusion(predicted, labels)+(epoch+1,))\n",
    "\n",
    "\n",
    "print('Finished Training')\n",
    "print(f'Time: {time.time() - start_time:.1f}s')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load model from file "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.plot(train_losses, label= \"Training losses\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_df = pd.DataFrame(test_accuracy,columns=['true_positives', 'false_positives', 'true_negatives', 'false_negatives','epoch'])\n",
    "res_df=res_df.groupby('epoch').agg({'true_positives':'sum', 'false_positives':'sum', 'true_negatives':'sum', 'false_negatives':'sum'}).reset_index()\n",
    "res_df['accuracy']=(res_df['true_positives']+res_df['true_negatives'])/X_test.shape[0]\n",
    "res_df['recall']=res_df['true_positives']/(res_df['true_positives']+res_df['false_negatives'])\n",
    "res_df['precission']=res_df['true_positives']/(res_df['true_positives']+res_df['false_positives'])\n",
    "res_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_df = pd.DataFrame(train_accuracy,columns=['true_positives', 'false_positives', 'true_negatives', 'false_negatives','epoch'])\n",
    "res_df=res_df.groupby('epoch').agg({'true_positives':'sum', 'false_positives':'sum', 'true_negatives':'sum', 'false_negatives':'sum'}).reset_index()\n",
    "res_df['accuracy']=(res_df['true_positives']+res_df['true_negatives'])/X_train.shape[0]\n",
    "res_df['recall']=res_df['true_positives']/(res_df['true_positives']+res_df['false_negatives'])\n",
    "res_df['precission']=res_df['true_positives']/(res_df['true_positives']+res_df['false_positives'])\n",
    "res_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Test the network on the test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correct = 0\n",
    "total = 0\n",
    "# since we're not training, we don't need to calculate the gradients for our outputs\n",
    "with torch.no_grad():\n",
    "    for data in testloader:\n",
    "        images, labels = data\n",
    "        images = images.to(device) #needed when using gpu\n",
    "        labels = labels.to(device) #needed when using gpu\n",
    "        \n",
    "        # calculate outputs by running images through the network\n",
    "        outputs = net(images.float())\n",
    "        # the class with the highest energy is what we choose as prediction\n",
    "        predicted = torch.argmax(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "print(f'Accuracy of the network on the test data: {100 * correct // total} %')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correct = 0\n",
    "total = 0\n",
    "# since we're not training, we don't need to calculate the gradients for our outputs\n",
    "with torch.no_grad():\n",
    "    for data in trainloader:\n",
    "        images, labels = data\n",
    "        images = images.to(device) #needed when using gpu\n",
    "        labels = labels.to(device) #needed when using gpu\n",
    "        \n",
    "        # calculate outputs by running images through the network\n",
    "        outputs = net(images.float())\n",
    "        # the class with the highest energy is what we choose as prediction\n",
    "        predicted = torch.argmax(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "print(f'Accuracy of the network on the train data: {100 * correct // total} %')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
